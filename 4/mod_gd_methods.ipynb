{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73688520-93c8-4bf9-8ece-69c6bfa08ca9",
   "metadata": {},
   "source": [
    "1. Implement:\n",
    "\n",
    "    (a) Momentum method;\n",
    "    \n",
    "    (b) Nesterov method;\n",
    "    \n",
    "    (c) AdaGrad method;\n",
    "    \n",
    "    (d) AdaDelta method;\n",
    "\n",
    "    (e) RMSProp method;\n",
    "   \n",
    "    (f) Adam method.\n",
    "3. For the study, consider:\n",
    "\n",
    "    (a) Well-conditioned (μ ≃1) two-dimensional quadratic function;\n",
    "   \n",
    "    (b) Ill-conditioned (μ > 10) two-dimensional quadratic function;\n",
    "   \n",
    "    (c) Rosenbrock function.\n",
    "4. For each function:\n",
    "\n",
    "    (a) Build a table that represents the dependence of the number of iterations of the gradient method, the number of function calculations (if any), and the number of function gradient calculations (if any) for each of the methods on the chosen accuracy;\n",
    "   \n",
    "    (b) plot the data from the table (abscissa axis – accuracy, ordinate axis – number of iterations/function/gradient calculations);\n",
    "5. For the Rosenbrock function, plot and compare the trajectories of each method on the level line plot.\n",
    "\n",
    "6. Implement a generator of quadratic functions of a given dimension and condition number. Investigate the dependence of the number of iterations that must be performed to achieve the selected accuracy, depending on these parameters. Consider this dependence for two gradient methods, one of which is (necessarily) the Adam method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c717d2-e1e4-43ce-b347-2d0742079a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065959bf-8292-435b-a858-4874b7e9c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f: Callable, x: np.ndarray):\n",
    "    Delta_f = np.zeros_like(x)\n",
    "    for k in range(len(x)):\n",
    "        dx = np.zeros_like(x)\n",
    "        dx[k] = np.finfo(float).eps * 10\n",
    "        Delta_f[k] = (f(x + dx) - f(x - dx)) / (2 * dx)\n",
    "    return Delta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6101a2-a78c-4464-a055-6b079988b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(\n",
    "    f: Callable[[np.ndarray], np.ndarray],\n",
    "    x0: np.ndarray,\n",
    "    lr: float,\n",
    "    maxit: int = 1000,\n",
    "    eps: float = np.finfo(float).eps * 10,\n",
    "    lamb=0.5,\n",
    ") -> np.ndarray:\n",
    "    x = x0\n",
    "    for _ in range(maxit):\n",
    "        g = grad(f, x)\n",
    "        if f(x) - f(x - lr * g) > eps * lr * np.linalg.norm(g):\n",
    "            x -= lr * g\n",
    "        else:\n",
    "            lr *= lamb\n",
    "            x -= lr * g\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
