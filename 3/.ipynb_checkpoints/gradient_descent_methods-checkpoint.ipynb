{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3. Gradient descent methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement:\n",
    "\n",
    "    (a) Constant-step gradient descent;\n",
    "    \n",
    "    (b) Gradient descent with fractional steps (Armijo condition);\n",
    "    \n",
    "    (c) Steepest descent (using any two one-dimensional search methods);\n",
    "    \n",
    "    (d) Conjugate gradient method (with restart).\n",
    "2. For the study, consider:\n",
    "\n",
    "    (a) Well-conditioned (μ ≃1) two-dimensional quadratic function;\n",
    "   \n",
    "    (b) Ill-conditioned (μ > 10) two-dimensional quadratic function;\n",
    "   \n",
    "    (c) Rosenbrock function.\n",
    "3. For each function:\n",
    "\n",
    "    (a) Build a table that reflects the dependence of the number of iterations of the gradient method, the number of function calculations(if any), and the number of function gradient calculations (if any) for each of the methods on the chosen accuracy;\n",
    "   \n",
    "    (b) plot the data from the table (abscissa axis – accuracy, ordinate axis – number of iterations/function/gradient calculations);\n",
    "4. For the Rosenbrock function, plot and compare the trajectories of each method on the level line plot.\n",
    "\n",
    "5. Implement a generator of quadratic functions of a given dimension and condition number. Investigate the dependence of the number of iterations that must be performed to achieve the selected accuracy, depending on these parameters. Consider this dependence for two gradient methods, one of which is (necessarily) the conjugate gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. \n",
    "\n",
    "Implement:\n",
    "\n",
    "(a) Constant-step gradient descent;\n",
    "\n",
    "(b) Gradient descent with fractional steps (Armijo condition);\n",
    "\n",
    "(c) Steepest descent (using any two one-dimensional search methods);\n",
    "\n",
    "(d) Conjugate gradient method (with restart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f: Callable, x: np.ndarray):\n",
    "    Delta_f = np.zeros_like(x)\n",
    "    for k in range(len(x)):\n",
    "        dx = np.zeros_like(x)\n",
    "        dx[k] = np.finfo(float).eps * 10\n",
    "        Delta_f[k] = (f(x + dx) - f(x - dx)) / (2 * dx)\n",
    "    return Delta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "def gd_constant_step(\n",
    "    f: Callable[[np.ndarray], np.ndarray], x0: np.ndarray, lr: float, maxit: int = 1000\n",
    ") -> np.ndarray:\n",
    "    x = x0\n",
    "    for _ in range(maxit):\n",
    "        x -= lr * grad(f, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "def gd_fractional_step(\n",
    "    f: Callable[[np.ndarray], np.ndarray],\n",
    "    x0: np.ndarray,\n",
    "    lr: float,\n",
    "    maxit: int = 1000,\n",
    "    eps: float = np.finfo(float).eps * 10,\n",
    "    lamb=0.5,\n",
    ") -> np.ndarray:\n",
    "    x = x0\n",
    "    for _ in range(maxit):\n",
    "        g = grad(f, x)\n",
    "        if f(x) - f(x - lr * g) > eps * lr * np.linalg.norm(g):\n",
    "            x -= lr * g\n",
    "        else:\n",
    "            lr *= lamb\n",
    "            x -= lr * g\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c\n",
    "def steepest_descent(\n",
    "    f: Callable[[np.ndarray], np.ndarray],\n",
    "    x0: np.ndarray,\n",
    "    max_lr: float,\n",
    "    method: str = \"golden_section\",\n",
    "    maxit: int = 1000,\n",
    "    eps: float = np.finfo(float).eps * 10,\n",
    ") -> np.ndarray:\n",
    "    '''golden section search or brent search'''\n",
    "\n",
    "    if method == \"golden_section\":\n",
    "\n",
    "        def golden_section_search(\n",
    "            f: Callable[[float], float], a: float, b: float, eps=1e-9, out_info=False\n",
    "        ):\n",
    "            iter_count, f_call_count = 0, 0\n",
    "            if b < a:\n",
    "                a, b = b, a\n",
    "            # golden ratio\n",
    "            gr = (5**0.5 - 1) / 2\n",
    "\n",
    "            iter_count += 1\n",
    "            f_call_count += 2\n",
    "            x = b - (b - a) * gr\n",
    "            y = a + (b - a) * gr\n",
    "            fx = f(x)\n",
    "            fy = f(y)\n",
    "            while b - a > eps:\n",
    "                iter_count += 1\n",
    "                f_call_count += 1\n",
    "                if fx < fy:\n",
    "                    b = y\n",
    "                    y = x\n",
    "                    fy = fx\n",
    "                    x = b - (b - a) * gr\n",
    "                    fx = f(x)\n",
    "                else:\n",
    "                    a = x\n",
    "                    x = y\n",
    "                    fx = fy\n",
    "                    y = a + (b - a) * gr\n",
    "                    fy = f(y)\n",
    "\n",
    "            res = (b + a) / 2\n",
    "            if not out_info:\n",
    "                return res\n",
    "            else:\n",
    "                return res, iter_count, f_call_count, b - a\n",
    "\n",
    "        search_method = golden_section_search\n",
    "    else:\n",
    "\n",
    "        def brent_search(\n",
    "            f: Callable[[float], float], a: float, b: float, eps=1e-9, out_info=False\n",
    "        ):\n",
    "            iter_count, f_call_count = 1, 1\n",
    "            if b < a:\n",
    "                a, b = b, a\n",
    "            K = (3 - 5**0.5) / 2\n",
    "            x = w = v = (a + b) / 2\n",
    "            fw = fv = fx = f(x)\n",
    "            deltax = 0.0\n",
    "            iter = 0\n",
    "            maxiter = 1e9\n",
    "            while iter < maxiter:\n",
    "                iter += 1\n",
    "                iter_count += 1\n",
    "\n",
    "                mintol = eps / 4\n",
    "                tol1 = eps * np.abs(x) + mintol\n",
    "                tol2 = 2.0 * tol1\n",
    "                xmid = 0.5 * (a + b)\n",
    "                # check for convergence\n",
    "                if np.abs(x - xmid) < (tol2 - 0.5 * (b - a)):\n",
    "                    break\n",
    "                if np.abs(deltax) <= tol1:\n",
    "                    if x >= xmid:\n",
    "                        deltax = a - x  # do a golden section step\n",
    "                    else:\n",
    "                        deltax = b - x\n",
    "                    rat = K * deltax\n",
    "                else:  # do a parabolic step\n",
    "                    tmp1 = (x - w) * (fx - fv)\n",
    "                    tmp2 = (x - v) * (fx - fw)\n",
    "                    p = (x - v) * tmp2 - (x - w) * tmp1\n",
    "                    tmp2 = 2.0 * (tmp2 - tmp1)\n",
    "                    if tmp2 > 0.0:\n",
    "                        p = -p\n",
    "                    tmp2 = np.abs(tmp2)\n",
    "                    dx_temp = deltax\n",
    "                    deltax = rat\n",
    "                    # determine whether a parabolic step is acceptable or not:\n",
    "                    if (\n",
    "                        (p > tmp2 * (a - x))\n",
    "                        and (p < tmp2 * (b - x))\n",
    "                        and (np.abs(p) < np.abs(0.5 * tmp2 * dx_temp))\n",
    "                    ):\n",
    "                        # whew, parabolic fit:\n",
    "                        rat = p * 1.0 / tmp2\n",
    "                        u = x + rat\n",
    "                        if (u - a) < tol2 or (b - u) < tol2:\n",
    "                            if xmid - x >= 0:\n",
    "                                rat = tol1\n",
    "                            else:\n",
    "                                rat = -tol1\n",
    "                    else:\n",
    "                        # nope, try golden section instead\n",
    "                        if x >= xmid:\n",
    "                            deltax = a - x\n",
    "                        else:\n",
    "                            deltax = b - x\n",
    "                        rat = K * deltax\n",
    "                # update current position:\n",
    "                if np.abs(rat) < tol1:  # update by at least tol1\n",
    "                    if rat >= 0:\n",
    "                        u = x + tol1\n",
    "                    else:\n",
    "                        u = x - tol1\n",
    "                else:\n",
    "                    u = x + rat\n",
    "                fu = f(u)\n",
    "                f_call_count += 1\n",
    "                if fu > fx:\n",
    "                    # Oh dear, point u is worse than what we have already,\n",
    "                    # even so it *must* be better than one of our endpoints:\n",
    "                    if u < x:\n",
    "                        a = u\n",
    "                    else:\n",
    "                        b = u\n",
    "\n",
    "                    if (fu <= fw) or (w == x):\n",
    "                        # however it is at least second best:\n",
    "                        v = w\n",
    "                        w = u\n",
    "                        fv = fw\n",
    "                        fw = fu\n",
    "                    elif (fu <= fv) or (v == x) or (v == w):\n",
    "                        # third best:\n",
    "                        v = u\n",
    "                        fv = fu\n",
    "                else:\n",
    "                    # good new point is an improvement!\n",
    "                    # update brackets:\n",
    "                    if u >= x:\n",
    "                        a = x\n",
    "                    else:\n",
    "                        b = x\n",
    "                    # update control points:\n",
    "                    v = w\n",
    "                    w = x\n",
    "                    x = u\n",
    "                    fv = fw\n",
    "                    fw = fx\n",
    "                    fx = fu\n",
    "            res = x\n",
    "            if not out_info:\n",
    "                return res\n",
    "            else:\n",
    "                return res, iter_count, f_call_count, b - a\n",
    "\n",
    "        search_method = brent_search\n",
    "    x = x0\n",
    "    for _ in range(maxit):\n",
    "        g = grad(f, x)\n",
    "        lr = search_method(lambda y: f(x - y * g), 0, max_lr, eps=eps)\n",
    "        x -= lr * g\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155456/4086046062.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Delta_f[k] = (f(x + dx) - f(x - dx)) / (2 * dx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32949812e-31]\n",
      "[9.69489299e-16]\n",
      "[1.28026745e-31]\n"
     ]
    }
   ],
   "source": [
    "def test_gd():\n",
    "    def f1(x):\n",
    "        return x**2\n",
    "\n",
    "    print(gd_constant_step(f1, np.random.random(1), 0.1))\n",
    "    assert np.isclose(gd_constant_step(f1, np.random.random(1), 0.1), 0)\n",
    "    print(gd_fractional_step(f1, np.random.random(1), 0.1))\n",
    "    assert np.isclose(gd_fractional_step(f1, np.random.random(1), 0.1), 0)\n",
    "    print(steepest_descent(f1, np.random.random(1), 0.1))\n",
    "    assert np.isclose(steepest_descent(f1, np.random.random(1), 0.1), 0)\n",
    "test_gd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
